---
subtitle: "Een analyse van kansengelijkheid in studiesucces ({{< meta params.model >}})"

# Format and output
output-file: "ch-models.html"

# Parameters
params:
  versie: "1.0"
  succes: "Retentie na 1 jaar"
  model: "Retentie na 1 jaar"
  pd: "Nvt"
  use_synthetic_data: true
  recreate_plots: false
  enrollment_selection: false
  sp: "CMD"
  sp_form: "VT"
  
# Content
includes:
  inleiding:      true
  model_lr:       true
  model_rf:       false
  model_svm:      false
  final_fit:      true
  conclusions:    true
  contact:        true
  justification:  true
  copyright:      true
---

<!-- Title -->

# Prognosemodel

```{r setup}
#| label: setup
#| include: false

# Current file
current_file <- "ch4-models.qmd"

# Include the _Setup.R file
source("_Setup.R") 

include_model_lr     <- rmarkdown::metadata$includes$model_lr    # Penalized Logistic Regression
include_model_rf     <- rmarkdown::metadata$includes$model_rf    # Random Forest
include_model_svm    <- rmarkdown::metadata$includes$model_svm   # Support Vector Machine
include_final_fit    <- rmarkdown::metadata$includes$final_fit   # Final Fit
include_conclusions  <- rmarkdown::metadata$includes$conclusions # Conclusies

df_model_results <- data.frame(
  model = character(),
  auc = numeric()
)
```

<!-- HEADER -->

<!-- Studyprogram short -->
{{< include R/qmd/header-studyprogram.qmd >}}

<!-- Methods -->

## Methode, data en analyse

### Toelichting op de methode

Voor de ontwikkeling van prognosemodellen gebruiken we de aanpak van [Tidymodels](https://www.tidymodels.org/). Tidymodels is een framework voor het bouwen van een prognosemodel. Hiermee verzekeren we ons van een systematische, herhaalbare en schaalbare aanpak. De code in deze pagina wordt getoond als deze hoort bij de ontwikkeling van een model. Overige code, voor bijvoorbeeld algemene tabellen of grafieken, kan zichtbaar gemaakt worden.

### Toelichting op de data

De basis voor deze analyse is studiedata van De Haagse Hogeschool (De HHs), verrijkt door het lectoraat LTA. De data bevat informatie over de inschrijvingen van studenten in het eerste jaar van de opleiding:

1.  *Demografische kenmerken*: geslacht, leeftijd, reistijd en SES totaalscore.
2.  *Vooropleidingskenmerken*: toelaatgevende vooropleiding, studiekeuzeprofiel en gemiddeld eindcijfer in de vooropleiding.
3.  *Aanmeldingskenmerken*: aansluiting (direct na diploma, tussenjaar, switch), dag van aanmelding, aantal parallelle studies aan De HHs en collegejaar.

De variabelen die we hebben geselecteerd voor analyse is op basis van standaardgegevens van een popoluatie (leeftijd en geslacht), eerdere analsyes op voorspelkracht voor `r tolower(params$succes)` [@Bakker.2022], of omdat een kenmerk volgens Europese normen geldt als een sensitief kenmerk voor discriminatie [@handboek.2018].

```{r}
#| label: tbl-data-dictionary
#| tbl-cap: "Variabelen en mogelijke waarden"

# Read the data dictionary
df_data_dictionary <- get_data_dictionary()

# Show the data dictionary
get_tbl_data_dictionary(df_data_dictionary)
  

```

### Toelichting op de analyse

We toetsen in deze analyse *`r succes_model_text`*, voortaan **Retentie** genoemd.

-   Retentie is gedefinieerd als ingeschreven staan in dezelfde opleiding in een aansluitend collegejaar. Een wisseling van opleidingsvorm binnen de opleiding, bijvoorbeeld van voltijd in jaar 1 naar duaal in jaar 2, geldt ook als retentie. 
-   Uitval is het tegenovergestelde van retentie: niet ingeschreven staan in dezelfde opleiding in een aansluitend collegejaar. 
-   Een wisseling van opleidingsvorm binnen de opleiding, bijvoorbeeld van voltijd in jaar 1 naar duaal in jaar 2, geldt *niet* als uitval.

<!-- Data -->

## Voorbereidingen

### Laad de data

We laden een subset in van historische data specifiek voor:

**Opleiding**: `r current_sp$INS_Faculteit` \| `r get_sp_name_syn(current_sp$INS_Opleidingsnaam_huidig)` (`r current_sp$INS_Opleiding`), `r current_sp$INS_Opleidingsvorm`, eerstejaars - **`r succes_model`**

```{r}
#| label: load-data

# Read the data for this study programme
if (params$use_synthetic_data) {
  
  print(params$sp)
  
  print(params$sp_form)
  
  df_sp_enrollments <- get_sp_enrollments_syn(sp = params$sp,
                                              sp_form = toupper(params$sp_form)) |> 
    mutate(
      ID = as.character(ID),
      INS_Opleiding = as.character(INS_Opleiding),
      INS_Opleidingsvorm = as.character(INS_Opleidingsvorm)
    ) |> 
    mutate(
      INS_Student_UUID_opleiding_vorm = paste(ID, INS_Opleiding, INS_Opleidingsvorm, sep = "_"),
      INS_Opleidingsnaam_huidig = paste(INS_Opleidingsnaam_huidig, "(Synth.)", sep = " ")
    )
} else {
  df_sp_enrollments <- 
    get_lta_studyprogram_enrollments_pin(board = "HHs/Inschrijvingen",
                                         faculty = params$faculty,
                                         studyprogram = params$sp,
                                         sp = params$sp,
                                         sp_form = toupper(params$sp_form),
                                         range = "eerstejaars")
}

# Adjust df_sp_enrollments
df_sp_enrollments <- df_sp_enrollments |> 
  
  # Rearrange the levels
  mutate(across(all_of(names(levels_formal)), 
                ~ factor(.x,
                         levels = levels_formal[[cur_column()]]))) |> 

  # Create a simple success variable
  mutate_retention(succes_model) |>
  
  # Convert the success variable into a factor
  mutate(SUC_Retentie = as.factor(SUC_Retentie)) |> 

  ## Special possibly based on the propaedeutic diploma
  # Filter_pd(pd) |>

  # Make the Dual Study variable a Yes/No variable
  mutate_parallel_sp() |>  

  # Remove INS_Aantal_inschrijvingen
  select(-INS_Aantal_inschrijvingen) 

## Adjust the levels of sensitive variables
for (i in sensitive_formal_variables){
  df_sp_enrollments <- df_sp_enrollments |>
    mutate_levels(
      i,
      list(levels_formal[[i]])
    )
}
  
# B Huidtherapie: Filter on only students with a grade number (selection)
if (sp == "HDT") {
  df_sp_enrollments <- df_sp_enrollments |> 
    filter(!is.na(RNK_Rangnummer)) 
} 

```

### Selecteer en inspecteer de data

We selecteren eerst de relevante variabelen. We verwijderen daarbij variabelen die maar één waarde hebben, omdat die geen voorspellende waarde kunnen hebben. 

```{r}
#| label: select-inspect-data

list_select <- get_list_select(df_variables, "VAR_Formal_variable")

# B Huidtherapie: add the variable RNK_Rangnummer unless it is HDT
if (sp == "HDT") {
  list_select <- c(list_select, "RNK_Rangnummer")
}

# Create a mapping with formal and simple maes
name_mapping_list <- setNames(df_variables$VAR_Simple_variable, df_variables$VAR_Formal_variable)

# Create a subset
df_sp_enrollments <- df_sp_enrollments |>
  
  # Select the relevant variables
  select(all_of(list_select)) |>
  
  # Rename variables for more readable names
  rename_with(~ name_mapping_list[.x], .cols = everything()) |> 
  
  # Adjust CBS_APCG_tf to a factor
  mutate_apcg() |>

  # Indicate where missing numbers are in VO
  mutate_grade_preeducation() |>
  
  # Remove variables, where there is only 1 value
  select(where(~ n_distinct(.) > 1)) |>
  
  # Sort
  arrange(Collegejaar, ID)

df_sp_enrollments <- df_sp_enrollments |> 
  sort_distinct()

```

We inspecteren de variabelen in een samenvatting in relatie tot retentie en corrigeren daarbij voor multiple testing; de gecorrigeerde significantie-waarden staan vermeld als *q-value*. 

```{r}
#| label: tbl-summary-data
#| tbl-cap: !expr 'glue("Variabelen in relatie tot de uitkomstmaat: {params$succes}")'
#| results: asis

# Create a summary of the data
df_summary <- df_sp_enrollments |>
  
  # Remove columns not relevant to the analysis
  select(-c(ID, Collegejaar)) |> 
  
  # Adjust the labels of Retention from True to Ja, and from False to Nee
  mutate(Retentie = forcats::fct_recode(Retentie, "Nee" = "FALSE", "Ja" = "TRUE")) |>
  
  # Adjust the order of the labels of Retentie
  mutate(Retentie = forcats::fct_relevel(Retentie, "Ja", "Nee")) |> 
  
  # Adjust Studiekeuzeprofiel: if NA, then unknown
  mutate(Studiekeuzeprofiel = coalesce(Studiekeuzeprofiel, "Onbekend")) |> 
  
  # Factor all character variables
  mutate(across(where(is.character), as.factor))

# Create a tbl_df of the summary
tbl_summary <- get_tbl_summary(df_summary) 

tbl_summary
  
```

Daarnaast inspecteren we de kwaliteit van de data op missende waarden.

```{r}
#| label: tbl-summarizy-missing-before
#| tbl-cap: "Kwaliteit van de data voor bewerkingen (gesorteerd op missende waarden)"

# Load dlookr (temporary to avoid conflicts)
suppressMessages(library(dlookr))

# Show a summary of the data, sorted by missing values
diagnose(df_sp_enrollments) |> 
  mutate(missing_percent = round(missing_percent, 2),
         unique_rate = round(missing_percent, 2)) |>
  arrange(desc(missing_percent)) |>
  knitr::kable(col.names = c("Variabelen",
                             "Type",
                             "# Missende waarden",
                             "% Missende waarden",
                             "# Unieke waarden",
                             "Ratio unieke waarden"))

# Detach dlookr
detach("package:dlookr", unload = TRUE)

```

### Bewerk de data

-   Uit de eerste diagnose blijkt dat niet alle variabelen goed genoeg zijn voor het bouwen van een prognosemodel: er zijn missende waarden en niet alle veldtypes zijn geschikt.
-   Om bias te voorkomen verwijderen we geen rijen met missende waarden, maar vullen die op (*imputatie*). We bewerken de data zo dat alle missende waarden worden opgevuld: bij numerieke waarden met het gemiddelde en bij categorische variabelen met 'Onbekend'.
-   We passen het type van sommige variabelen aan, zodat ze in het model gebruikt kunnen worden: tekstvelden zetten we om naar factor (een categorische variabele); logische variabelen (Ja/Nee) zetten we om naar een numerieke variabele (1/0).
-   De uitkomstvariabele, `Retentie`, leiden we af van de variabele `SUC_Uitval_aantal_jaar_LTA`. Als de waarde daar 1 is, is de student na 1 jaar uitgevallen, 2 na 2 jaar, etc. Zolang de waarde daar 0 is, is de student niet uitgevallen.
-   Een fictief studentnummer (`INS_Student_UUID_opleiding_vorm`) gebruiken we, zodat we -- als er afwijkende resultaten zijn -- de dataset gericht kunnen onderzoeken als dat nodig is.

```{r}
#| label: tbl-summarizy-missing-after
#| tbl-cap: "Kwaliteit van de data na bewerkingen (gesorteerd op missende waarden)"

# Edit the data
df_sp_enrollments <- df_sp_enrollments |> 
  
  # Imputate all numeric variables with the mean
  mutate(across(where(is.numeric), ~ ifelse(
    is.na(.x),
    mean(.x, na.rm = TRUE),
    .x
  ))) |>
  
  # Convert character variables to factor
  mutate(across(where(is.character), as.factor)) |> 
  
  # Convert logical variables to 0 or 1
  mutate(across(where(is.logical), as.integer)) |>
  
  # Fill in factors missing values with “Unknown”
  mutate(across(where(is.factor), ~ suppressWarnings(
    forcats::fct_explicit_na(.x, na_level = "Onbekend")
  ))) |> 
  
  # Rearrange the columns so that Retentie is in front
  select(Retentie, everything()) 

# Load dlookr (temporary to avoid conflicts)
suppressMessages(library(dlookr))

# Diagnose the data
diagnose(df_sp_enrollments) |> 
  mutate(missing_percent = round(missing_percent, 2),
         unique_rate = round(unique_rate, 2)) |>
  knitr::kable(col.names = c("Variabelen",
                             "Type",
                             "# Missende waarden",
                             "% Missende waarden",
                             "# Unieke waarden",
                             "Ratio unieke waarden"))

# Detach dlookr
detach("package:dlookr", unload = TRUE)

```

### Inspecteer de onderlinge correlaties

Het is verstandig om voorafgaand aan het bouwen van een model te kijken naar de onderlinge correlaties tussen numerieke variabelen. Dit geeft inzicht in de data en kan helpen bij het maken van keuzes voor het model of de duiding van de uitkomsten.

We selecteren uitsluitend numerieke waarden en variabelen die een standaard deviatie hebben die groter is dan 0. We clusteren de data op basis van 4 clusters. De correlatie van de diagonaal verbergen we, aangezien deze altijd 1 is.

Clusters in correlaties kunnen per opleiding verschillen. Onderzoek in de correlaties hoe sterk deze zijn, welke clusters gevormd worden en of deze -- vanuit de context van de opleiding, faculteit of onderwijsinstelling -- logisch zijn. Verwerk de inzichten eventueel in een oplegger. Gangbare clusters zijn:

-   Cijfers: Een correlatie tussen Cijfer SO en Cijfer VO: dit geeft aan dat het schoolexamen hetzelfde meet als het centraal examen. Exacte vakken en talen kunnen met elkaar correleren.
-   Reistijd en SES: Deze zijn gecorreleerd aangezien SES scores zijn afgeleid van wijken. Wijken hebben een specifieke reistijd en positionering ten opzichte van de vestigingslocaties van De HHs kennen.
-   Leeftijd is vaak niet gecorreleerd met andere variabelen.

```{r}
#| label: fig-corplot-data
#| fig-cap: "Correlatiematrix"

# Create a plot of the intercorrelations in numerical variables
# Remove columns with a standard deviation of 0
df_correlation <- df_sp_enrollments |> 
  select(-Collegejaar) |>
  select(where(is.numeric)) |> 
  select_if(~ sd(.) != 0) |>
  cor()

df_correlation |>  
  corrplot::corrplot(order = "hclust", 
                     addrect = 4,
                     method = "number",
                     hclust.method = "complete",
                     tl.cex = 0.8,       
                     tl.col = "black",
                     diag = FALSE)

```

```{r}
#| label: fig-corplot-dendrogram
#| fig-cap: "Dendrogram van de correlatiematrix"

# Apply hierarchical clustering
dist   <- as.dist(1 - df_correlation)      # Create a distance matrix
hclust <- hclust(dist, method = "complete")  # Hierarchical clustering

# Create a clustering
df_clusters <- cutree(hclust, k = 4)

# Show clustering
# df_clusters

```

### Bouw de trainingset, validatieset en testset

-   De data is nu geschikt om een prognosemodel mee te bouwen.
-   Om het model te bouwen, testen en valideren, splitsen we de data in drie delen van 60%, 20% en 20%. We doen dit op zo'n manier, dat elk deel ongeveer een gelijk aantal studenten bevat dat doorstudeert (dus niet uitvalt).
-   We trainen het model op basis van 60% en valideren de modellen tijdens het trainen op de overige 20% (de validatieset).
-   De verdeling van de training- en validatieset muteren we 10x (10 *folds*) om te voorkomen dat het model te veel leert van de trainingset en daardoor slecht presteert op de validatieset.
-   Als het model klaar is, testen we het op de 20% studenten uit de testset. De testset blijft dus de gehele tijd ongemoeid, zodat we overfitting - een te goed model op bekende data, maar slechte presetaties (*performance*) op onbekende data - voorkomen.
-   Een willekeurig, maar vaststaand *seed*-getal voorkomt dat we bij elke run van het model c.q. deze code een net iets andere uitkomst krijgen.

We ontwikkelen in de verdere analyse eerst een aantal modellen en testen de prestaties daarvan op de trainingset en validiatieset. Vervolgens bepalen we welk model het beste presteert en passen dat model dan toe op de testset (de uiteindelijke fit). Vandaar dat het toetsen van de prestaties van de modellen meerdere keren behandeld wordt.

```{r}
#| label: fig-dataset-split
#| fig-cap: "Splitsing van de dataset in trainingset, validatieset en testset"
#| out-width: 80%
knitr::include_graphics(here::here("R/images", "prognosemodel-dataset-lta-hhs.png"))
```


```{r}
#| label: split-data

set.seed(0821)

# Split the data into 3 parts: 60%, 20% and 20%
splits      <- initial_validation_split(df_sp_enrollments,
                                        strata = Retentie,
                                        prop = c(0.6, 0.2))

# Create three sets: a training set, a test set and a validation set
df_retention_train      <- training(splits)
df_retention_test       <- testing(splits)
df_retention_validation <- validation_set(splits)

# Create a resample set based on 10 folds (default)
df_retention_resamples  <- vfold_cv(df_retention_train, strata = Retentie)
```

```{r}
#| label: tbl-split-data
#| tbl-cap: "Verhouding van de uitkomstvariabele in de training- en testset"
#| echo: false

# Training set proportions
df_retention_train_prop <- df_retention_train |> 
  count(Retentie) |> 
  mutate(Naam = "Trainingset",
         prop = n / sum(n)) 

# Test set proportions
df_retention_test_prop <- df_retention_test  |> 
  count(Retentie) |> 
  mutate(Naam = "Testset",
         prop = n / sum(n)) 

# Combine the training and test set to display in a table
bind_rows(df_retention_train_prop,
          df_retention_test_prop) |> 
  mutate(prop = scales::percent(prop, accuracy = 0.1)) |>
  select(Naam, Retentie, n, prop) |> 
  knitr::kable(col.names = c("Naam", "Retentie", "Aantal", "Proportie"))

```

<!-- MODEL I: Penalized Logistic Regression -->

::: {.content-hidden unless-meta="includes.model_lr"}
## Model I: Logistische Regressie

-   Het eerste model is een [logistische regressie met penalized likelihood](https://wikistatistiek.amc.nl/Logistische_regressie); we gebruiken de `glmnet` engine voor het bouwen van het model. Penalized likelihood is een techniek die helpt bij het voorkomen van overfitting: het voegt voor elke extra variabele een strafterm toe om eenvoudige modellen te belonen. [Glmnet](https://glmnet.stanford.edu/articles/glmnet.html) is een veelgebruikt package voor het bouwen van logistische regressiemodellen.
-   We gebruiken de [Area under the ROC Curve (AUC/ROC)](https://nl.wikipedia.org/wiki/ROC-curve) als performance metric. De ROC-curve (Receiver Operating Characteristic) is een grafiek die de prestaties van een classificatiemodel afbeeldt door de verhouding tussen de *true positives* (sensitiviteit) en de *false positives* (aspecficiteit = 1-specificiteit) te plotten bij verschillende drempelwaarden. De oppervlakte onder deze curve, bekend als de AUC (Area Under the Curve), kwantificeert het onderscheidingsvermogen van het model; een AUC van 1 duidt op een perfect onderscheidend vermogen, terwijl een AUC van 0,5 wijst op een model zonder onderscheidend vermogen.

### Maak het model

Eerst bouwen we het model.

```{r}
#| label: lr-mod
#| code-fold: false

# Build the model: logistic regression
lr_mod <- 
  logistic_reg(penalty = tune(), mixture = 1) |> 
  set_engine("glmnet")
```

### Maak de recipe

Vervolgens zetten we meerdere stappen in een 'recipe':

-   We definiëren de student-ID als ID variabele. Daarmee krijgt deze variabele de rol van uniek rij-kenmerk.
-   We verwijderen vervolgens de oorspronkelijke student-ID en het collegejaar uit de data, omdat deze verder niet gebruikt moeten worden in het model.
-   We converteren factoren naar dummy variabelen: voor elke categorie wordt er een nieuwe logische variabele (Ja/Nee) aangemaakt.
-   We verwijderen variabelen die geen waarde toevoegen: variabelen met uitsluitend nullen.
-   We normaliseren numerieke variabelen om ze met elkaar te kunnen vergelijken door ze te centreren en schalen: het transformeert numerieke gegevens zodat ze een standaard deviatie van één en een gemiddelde van nul hebben.
-   Sterk gecorreleerde waarden verwijderen we nu niet, omdat we later in de analyse de eventuele samenhang met andere variabelen in een prognosemodel nog willen kunnen visualiseren.

```{r}
#| label: lr-recipe
#| code-fold: false

# Build the recipe: logistic regression
lr_recipe <- 
  recipe(Retentie ~ ., data = df_retention_train) |>  
  update_role(ID, new_role = "ID") |>           # Set the student ID as an ID variable
  step_rm(ID, Collegejaar) |>                   # Remove ID and college year from the model
  step_unknown(Studiekeuzeprofiel, 
               new_level = "Onbekend skp") |>   # Add unknown skp
  step_dummy(all_nominal_predictors()) |>       # Create dummy variables from categorical variables
  step_zv(all_predictors()) |>                  # Remove zero values
  step_normalize(all_numeric_predictors())      # Center and scale numeric variables

```

```{r}
#| label: tbl-lr-recipe-steps
#| tbl-cap: "Recipesteps voor logistische regressie"

# Show the recipe
tidy(lr_recipe) |> 
  knitr::kable(col.names = c("Nummer", 
                             "Operatie", 
                             "Type",
                             "Getraind",
                             "Sla over",
                             "ID"))
```

De variabelen die nu nog overblijven zijn:

```{r}
#| label: tbl-lr-recipe-final
#| tbl-cap: "Resterende variabelen voor logistische regressie na bewerkingen"
#| echo: false

# Show the variables remaining
model_vars <- lr_recipe |> 
  prep() |> 
  juice() |> 
  names()

# Add empty values to make the length divisible by 3
while (length(model_vars) %% 3 != 0) {
  model_vars <- c(model_vars, "")
}

model_vars_matrix <- matrix(model_vars, ncol = 3, byrow = FALSE)

knitr::kable(model_vars_matrix)

```

### Maak de workflow

Voor de uitvoering bouwen we een workflow. Daaraan voegen we het model en de bewerkingen in de recipe toe.

```{r}
#| label: lr-workflow
#| code-fold: false

# Create the workflow: logistic regression
lr_workflow <- 
  workflow() |>         # Create a workflow
  add_model(lr_mod) |>  # Add the model
  add_recipe(lr_recipe) # Add the recipe

# Show workflow
lr_workflow
```

### Tune en train het model

Het model moet getuned worden. Dit houdt in dat we de beste parameters voor het model moeten vinden. We maken een grid met verschillende penalty waarden. Daarmee kunnen we vervolgens het beste model selecteren met de hoogste ROC/AUC. We plotten de resultaten van de tuning, zodat we hieruit het beste model kunnen kiezen.

```{r}
#| label: lr-reg-grid
#| code-fold: false

# Create a grid: logistic regression
lr_reg_grid <- tibble(penalty = 10 ^ seq(-4, -1, length.out = 30))

# Train and tune the model: logistic regression
lr_res <- 
  lr_workflow |> 
  tune_grid(df_retention_validation,
            grid = lr_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

```

```{r}
#| label: fig-lr-plot
#| fig-cap: "Tuning resultaten logistische regressie"

# Plot the results + a red vertical line for the max AUC
lr_plot <- 
  lr_res |> 
  collect_metrics() |> 
  ggplot(aes(x = penalty, y = mean)) + 
  geom_point() + 
  geom_line() + 
  
  # Make the scale of the x-axis logarithmic
  scale_x_log10(labels = scales::label_number()) +
  theme(
    axis.title.x = element_text(margin = margin(t = 20))
  ) +
  
  # Define the title, subtitle and caption
  labs(
    caption = caption,
    x = "Penalty",
    y = "Area under the ROC Curve"
  )
  
# Add theme elements
lr_plot <- add_theme_elements(lr_plot, title_subtitle = FALSE)
  
# Find the penalty value with the max AUC
max_auc_penalty <- lr_res |> 
  collect_metrics() |> 
  filter(mean == max(mean)) |> 
  pull(penalty)

# Add the red vertical line to lr_plot
lr_plot_plus <- lr_plot + 
  geom_vline(xintercept = max_auc_penalty, color = "red")

# Find a mean for the max AUC that is higher
max_auc_mean <- lr_res |> 
  collect_metrics() |> 
  filter(mean == max(mean)) |> 
  pull(penalty)

# Print the final plot
lr_plot_plus

```

### Kies het beste model

De prestaties van een model gevisualiseerd met behulp van een ROC curve. De sensitiviteit (True Positive Rate) en specificiteit (True Negative Rate) worden hierin tegenover elkaar uitgezet. De Area under the ROC Curve (AUC/ROC) geeft de prestaties van het model weer. Het model scoort beter naarmate de AUC/ROC dichter bij de 1 ligt, de linker bovenhoek. De linker bovenhoek houdt in dat alle prognoses exact overeenstemmen met de werkelijkheid. Een AUC/ROC van 0,5 betekent dat het model niet beter presteert dan een willekeurige voorspelling.

We gebruiken modellen met een zo hoog mogelijke Area under the ROC Curve (AUC/ROC) en een zo laag mogelijke penalty. Zo kunnen we uit de resultaten het beste model kiezen en visualiseren.

```{r}
#| label: lr-top-models
#| code-fold: false

# Show the best model
top_models <-
  lr_res |> 
  show_best(metric = "roc_auc", n = 10) |> 
  mutate(mean = round(mean, 6)) |>
  arrange(penalty) 
```

```{r}
#| label: tbl-lr-top-models
#| tbl-cap: "Model performance voor logistische regressie"

top_models |> 
  knitr::kable(col.names = c("Penalty", 
                             "Metriek", 
                             "Estimator",
                             "Gemiddelde",
                             "Aantal",
                             "SE",
                             "Configuratie"))

```

```{r}
#| label: lr-best
#| code-fold: false

# Select the best model: logistic regression
lr_best <- 
  lr_res |> 
  collect_metrics() |> 
  filter(mean == max(mean)) |>
  slice(1) 
```

```{r}
#| label: tbl-lr-best
#| tbl-cap: "Hoogste model performance voor logistische regressie"

lr_best |> 
  mutate(mean = round(mean, 6)) |>
  knitr::kable(col.names = c("Penalty", 
                             "Metriek", 
                             "Estimator",
                             "Gemiddelde",
                             "Aantal",
                             "SE",
                             "Configuratie"))

```

```{r}
#| label: lr-auc
#| code-fold: false

# Collect the predictions and evaluate the model (AUC/ROC): logistic regression
lr_auc <- 
  lr_res |> 
  collect_predictions(parameters = lr_best) |> 
  roc_curve(Retentie, .pred_FALSE) |> 
  mutate(model = "Logistisch Regressie")
```
```{r}
#| label: fig-lrauc
#| fig-cap: "ROC curve voor logistische regressie"

# Plot the ROC curve
get_roc_plot(lr_auc, position = 1)
```

```{r}
#| label: lr-auc-highest

# Determine the AUC of the best model
lr_auc_highest   <-
  lr_res |>
  collect_predictions(parameters = lr_best) |> 
  roc_auc(Retentie, .pred_FALSE)

# Add model name and AUC df_model_results
df_model_results <- 
  df_model_results |>
  add_row(model = "Logistic Regression", auc = lr_auc_highest$.estimate)

```
:::

<!-- MODEL II: Random Forest -->

::: {.content-hidden unless-meta="includes.model_rf"}
## Model II: Tree-based ensemble

-   Het tweede model is een [random forest](https://en.wikipedia.org/wiki/Random_forest): een ensemble van beslisbomen (*decision trees*). Het is een krachtig model dat goed om kan gaan met complexe data en veel variabelen.
-   We gebruiken de [`ranger` engine](https://cran.r-project.org/web/packages/ranger/index.html) voor het bouwen van het model.

### Bepaal het aantal PC-cores

Omdat een random forest model veel berekeningen vereist, willen we daarvoor alle computerkracht gebruiken die beschikbaar is. Het aantal CPU's (*cores*), wat verschilt per computer, bepaalt hoe snel het model getraind kan worden. We bepalen het aantal cores en gebruiken dat bij het bouwen van het model.

```{r}
#| label: cores

# Determine the number of cores
cores <- parallel::detectCores()

```

### Maak het model

We bouwen eerst het model. We gebruiken de `rand_forest` functie om het model te bouwen. We tunen de `mtry` en `min_n` parameters. De `mtry` parameter bepaalt het aantal variabelen dat per boom wordt gebruikt. De `min_n` parameter bepaalt het minimum aantal observaties dat in een blad van de boom moet zitten. De functie `tune()` is hier nog een *placeholder* om de beste waarden voor deze parameters - die we later bepalen - in te kunnen stellen. We gebruiken 1.000 bomen c.q. versies van het model.

```{r}
#| label: rf-mod
#| code-fold: false

# Build the model: random forest

rf_mod <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) |> 
  set_engine("ranger", num.threads = cores) |> 
  set_mode("classification")
```

### Maak de recipe

We maken een recipe voor het random forest model. We verwijderen de student ID en het collegejaar uit de data, omdat deze niet moet worden gebruikt in het model. Overige stappen zijn bij een random forest minder relevant in tegenstelling tot een regressiemodel.

```{r}
#| label: rf-recipe
#| code-fold: false

# Create the recipe: random forest
rf_recipe <- 
  recipe(Retentie ~ ., data = df_retention_train) |> 
  step_unknown(Studiekeuzeprofiel, 
               new_level = "Onbekend skp") |>   # Add unknown skp
  step_rm(ID, Collegejaar)                      # Remove ID and Collegejaar from the model
```

```{r}
#| label: tbl-rf-recipe
#| tbl-cap: "Recipesteps voor random forest"

# Show the recipe
tidy(rf_recipe) |> 
  knitr::kable(col.names = c("Nummer", 
                             "Operatie", 
                             "Type",
                             "Getraind",
                             "Sla over",
                             "ID"))
```

### Maak de workflow

We voegen het model en de recipe toe aan de workflow voor dit model.

```{r}
#| label: rf-workflow
#| code-fold: false

# Create the workflow: random forest
rf_workflow <- 
  workflow() |> 
  add_model(rf_mod) |> 
  add_recipe(rf_recipe)

# Show workflow
rf_workflow
```

### Tune en train het model

We trainen en tunen het model in de workflow. We maken een grid met verschillende waarden voor de parameters `mtry` en `min_n`. We gebruiken de Area under the ROC Curve (AUC/ROC) als performance metric. Met de resultaten van de tuning kiezen we het beste model.

```{r}
#| label: rf-tune
#| code-fold: false

# Show the parameters that can be tuned
rf_mod

# Extract the parameters being tuned
extract_parameter_set_dials(rf_mod)

# Determine the seed
set.seed(2904)

# Build the grid: random forest
rf_res <- 
  rf_workflow |> 
  tune_grid(df_retention_validation,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))
```

### Kies het beste model

We evalueren de beste modellen en maken een ROC curve om de performance van het model te visualiseren. Vervolgens vergelijken we de prestaties van de modellen en kiezen daaruit het beste model.

```{r}
#| label: tbl-rf-results
#| tbl-cap: "Model performance voor random forest"
#| code-fold: false

# Show the best models
rf_res |> 
  show_best(metric = "roc_auc", n = 15) |> 
  mutate(mean = round(mean, 6)) |>
  knitr::kable(col.names = c("Mtry", 
                             "Min. aantal", 
                             "Metriek",
                             "Estimator",
                             "Gemiddelde",
                             "Aantal",
                             "SE",
                             "Configuratie"))

```
```{r}
#| label: fig-rf-results
#| fig-cap: "Model performance random forest"

# Plot the results
autoplot <- autoplot(rf_res) +
  theme_minimal() +
  labs(
    y = "roc/auc",
    caption = caption
  )
  
# Add theme elements
autoplot <- add_theme_elements(autoplot, title_subtitle = FALSE)

print(autoplot)

```

```{r}
#| label: rf-best
#| code-fold: false

# Select the best model
rf_best <- 
  rf_res |> 
  select_best(metric = "roc_auc")

```

```{r}
#| label: tbl-ref-best
#| tbl-cap: "Hoogste model performance voor random forest"

rf_best |> 
  knitr::kable(col.names = c("Mtry", 
                             "Min. aantal", 
                             "Configuratie"))

```

```{r}
#| label: tbl-rf-predictions
#| tbl-cap: "Predicties voor random forest"

# Collect the predictions
rf_res |> 
  collect_predictions() |> 
  head(10) |>
  mutate(.pred_FALSE = scales::percent(.pred_FALSE, accuracy = 0.1),
         .pred_TRUE = scales::percent(.pred_TRUE, accuracy = 0.1)) |>
  knitr::kable(col.names = c("% Voorsp. FALSE", 
                             "% Voorsp. TRUE", 
                             "ID",
                             "Rij",
                             "Mtry", 
                             "Min. aantal", 
                             "Retentie",
                             "Configuratie"))
```

```{r}
#| label: fig-rf-auc
#| fig-cap: "ROC curve voor random forest"
 
# Determine the AUC/ROC curve
rf_auc <- 
  rf_res |> 
  collect_predictions(parameters = rf_best) |> 
  roc_curve(Retentie, .pred_FALSE) |> 
  mutate(model = "Random Forest")

# Plot the ROC curve
get_roc_plot(rf_auc, position = 2)

# Determine the AUC of the best model
rf_auc_highest   <-
  rf_res |>
  collect_predictions(parameters = rf_best) |> 
  roc_auc(Retentie, .pred_FALSE)

# Add model name and AUC to df_model_results
df_model_results <- 
  df_model_results |>
  add_row(model = "Random Forest", 
          auc = rf_auc_highest$.estimate)

```
:::

<!-- Final Fit -->

## De uiteindelijke fit

-   In de laatste stap van deze analyse maken we het model definitief.
-   We testen het model op de testset en evalueren het model met metrieken en de Variable Importance (VI). De VI kwantificeert de bijdrage van elke variabele aan de voorspellende kracht van een model. Het identificeert welke variabelen significant zijn voor de modelprestaties, wat essentieel is voor het interpreteren en optimaliseren van een model [@VanderLaan.2006]. Methoden zoals de Shapley-waarde en permutation importance worden vaak toegepast om dit belang te meten. Op deze methoden komen we terug in het volgende hoofdstuk.

### Combineer de AUC/ROC curves en kies het beste model

Eerst combineren we de AUC/ROC curves van de modellen om ze te vergelijken. We kiezen het beste model op basis van de hoogste AUC/ROC.

```{r}
#| label: fig-bind-rows-auc-roc
#| fig-cap: "Gecombineerde ROC curves"

# Combine the AUC/ROC curves to compare the models
get_roc_plot(list(lr_auc, rf_auc))

```

```{r}
#| label: best-model-auc-roc

# Determine which of the models is best based on highest AUC/ROC
df_model_results <- df_model_results |>
  mutate(number = row_number()) |> 
  mutate(best = ifelse(auc == max(auc), TRUE, FALSE)) |> 
  arrange(number)

# Determine the best model
best_model     <- df_model_results$model[df_model_results$best == TRUE]
best_model_auc <- round(df_model_results$auc[df_model_results$best == TRUE], 4)
```

```{r}
#| label: best-model-auc-roc-text
#| echo: false
#| results: asis

# Build the text for the best models
best_model_text <- df_model_results |> 
  filter(best == FALSE) |> 
  glue_data("Het {model} model heeft een AUC van {round(auc, 4)}. ") |> 
  stringr::str_flatten()

# Rearrange the final text for a clear summary
best_model_text <-
  glue::glue(
    "Het beste model is het **{best_model}** model met een **AUC/ROC van {best_model_auc}**. ",
    "{best_model_text} We ronden de analyse verder af met het {best_model} model ",
    "op de validatieset."
  )

```

`r best_model_text`

### Maak het finale model

```{r}
#| label: text-final-model
#| echo: false
#| results: asis

# Build the text for the final model
if (best_model == "Logistisch Regressie") {
  
  # Make the final model
  final_model_text <- glue::glue("We maken het finale model op basis van de beste parameters ",
                           "die we hebben gevonden. Voor het Logistisch Regressie model ",
                           "is dit het model met de beste penalty en mixture.")

} else if (best_model == "Random Forest") {
  
  # Make the final model
  final_model_text <- glue::glue("We maken het finale model op basis van de beste parameters ",
                           "die we hebben gevonden. Door in de engine bij `importance` ",
                           "de `impurity` op te geven, wordt het beste random forest model ",
                           "gekozen om de data definitief mee te classificeren.")

}
```

We maken het finale model op basis van de beste parameters die we hebben gevonden. Door in de engine bij `importance` de `impurity` op te geven, wordt het beste random forest model gekozen om de data definitief mee te classificeren.

```{r}
#| label: last-mod
#| code-fold: false

# Test the developed model on the test set
# Determine the optimal parameters

# Build the final models
last_lr_mod <-
  logistic_reg(penalty = lr_best$penalty,
               mixture = 1) |>
  set_engine("glmnet") |>
  set_mode("classification")

last_rf_mod <-
  rand_forest(mtry = rf_best$mtry,
              min_n = rf_best$min_n,
              trees = 1000) |>
  set_engine("ranger", num.threads = cores, importance = "impurity") |>
  set_mode("classification")

```

### Maak de workflow

We voegen het model toe aan de workflow en updaten de workflow met het finale model.

```{r}
#| label: last-workflow
#| code-fold: false

# Update the workflows
last_lr_workflow <- 
  lr_workflow |> 
  update_model(last_lr_mod)

last_rf_workflow <- 
  rf_workflow |> 
  update_model(last_rf_mod)

```

### Fit het finale model

We voeren de finale fit uit. De functie `last_fit` past het model toe op de validatieset.

```{r}
#| label: last-fit
#| code-fold: false

# Perform the final fit
set.seed(2904)

# Make a final fit for both models so we can save it for later use
last_fit_lr <- 
  last_lr_workflow |> 
  last_fit(splits)

last_fit_rf <- 
  last_rf_workflow |> 
  last_fit(splits)

last_fits <- list(last_fit_lr, last_fit_rf) |> 
  set_names(c("Logistic Regression", "Random Forest"))

# Determine which model is best
if (best_model == "Logistic Regression") {
  last_fit <- last_fit_lr
} else if (best_model == "Random Forest") {
  last_fit <- last_fit_rf
}

# Keep results, model results and associated data
fittedmodels_outputpath <- get_model_outputpath(mode = "last-fits")
saveRDS(last_fits, file = fittedmodels_outputpath)

modelresults_outputpath <- get_model_outputpath(mode = "modelresults")
saveRDS(df_model_results, file = modelresults_outputpath)

data_outputpath <- get_model_outputpath(mode = "data")
saveRDS(df_sp_enrollments, file = data_outputpath)

```

### Evalueer het finale model: metrieken en variable importance

We evalueren het finale model nu grondiger op basis van 4 metrieken: 1) accuraatheid, 2) ROC/AUC en 3) de [Brier score](https://en.wikipedia.org/wiki/Brier_score) (de Mean Squared Error). Het is zinvol om accuraatheid, ROC/AUC en de Brier-score pas bij het finale model toe te passen, omdat dit efficiënter is en overfitting voorkomt. Zo combineren we een snelle modelist_selectie met een grondige evaluatie van het uiteindelijke model.

```{r}
#| label: last-fit-metrics-vi
#| code-fold: false

# Collect the metrics
last_fit |> 
  collect_metrics() |> 
  mutate(.estimate = round(.estimate, 4)) |>
  knitr::kable(col.names = c("Metriek", 
                             "Estimator",
                             "Estimate",
                             "Configuratie"))

```

### Plot de ROC curve

Tot slot visualiseren we de prestaties weer met een ROC curve van het beste model.

```{r}
#| label: fig-last-fit-roc
#| fig-cap: "ROC curve finale model"
#| code-fold: false

# Show the roc curve
auc_lf <- last_fit |> 
  collect_predictions() |> 
  roc_curve(Retentie, .pred_FALSE) |> 
  mutate(model = "Last fit")

get_roc_plot(auc_lf, position = 3)

```

<!-- Conclusions -->

## Conclusies

```{r}
#| label: conclusions-accuracy
#| code-fold: false
#| echo: false

# Determine model accuracy, average retention rate and base model
last_fit_accuracy <- last_fit |>
  collect_metrics() |>
  filter(.metric == "accuracy") |>
  pull(.estimate) |>
  round(4) * 100
avg_retention        <- round(mean(df_sp_enrollments$Retentie == TRUE) * 100, 2)
avg_non_retention    <- round(100 - avg_retention, 2)

if (avg_retention < 50) {
  base_model_accuracy <- round(100 - avg_retention, 2)
  retention_text      <- "die niet doorstudeerde"
} else {
  base_model_accuracy <- avg_retention
  retention_text      <- "die doorstudeerde"
}

# Now calculate the difference in accuracy
accuracy_difference_number      <- round(abs(last_fit_accuracy - base_model_accuracy), 2)
accuracy_difference_percentage  <- paste0(accuracy_difference_number, "%") 

# Features
get_accuracy_comparison <-
  function(last_fit_accuracy, base_model_accuracy) {
    if (last_fit_accuracy == base_model_accuracy) {
      "even goed als"
    } else if (last_fit_accuracy > base_model_accuracy) {
      "beter"
    } else {
      "slechter"
    }
  }
  
get_accuracy_level <- function(last_fit_accuracy) {
  if (last_fit_accuracy > 95) {
    "zeer hoog"
  } else if (last_fit_accuracy > 90) {
    "hoog"
  } else if (last_fit_accuracy > 80) {
    "vrij hoog"
  } else if (last_fit_accuracy > 70) {
    "gemiddeld"
  } else if (last_fit_accuracy > 60) {
    "vrij laag"
  } else {
    "laag"
  }
}
  
get_accuracy_difference <- function(accuracy_difference_number) {
  if (accuracy_difference_number < 5) {
    "iets"
  } else if (accuracy_difference_number < 10) {
    "wat"
  } else if (accuracy_difference_number < 20) {
    "een stuk"
  } else {
    "veel"
  }
}

# Functions to create sentences and print variables
special_paste  <- function(vec) sub(",\\s+([^,]+)$", " en \\1", toString(vec))
print_variable <- function(variable) paste0("`", variable, "`")

# Determine a number of texts
accuracy_comparison  <- get_accuracy_comparison(last_fit_accuracy, base_model_accuracy)
accuracy_level       <- get_accuracy_level(last_fit_accuracy)
accuracy_difference  <- get_accuracy_difference(accuracy_difference_number)

# Determine the top 3 and top 5 variables based on the VI
df_top_vi   <- last_fit |> 
  extract_fit_parsnip() |> 
  vip::vi() |> 
  arrange(desc(Importance)) 

top3_vi_list <- df_top_vi |>
  slice(1:3) |>
  pull(Variable) |>
  # Adjust the variables so that they are between backticks
  print_variable() |>
  # Make it a sentence
  special_paste()  

top45_vi_list <- df_top_vi |>
  # Lines 4 and 5
  slice(4:5) |>
  pull(Variable) |>
  # Adjust the variables so that they are between backticks
  print_variable() |>
  # Make it a sentence
  special_paste()

conclusion_text <- ""

if (!("Eindcijfer_VO" %in% df_top_vi$Variable[1:3]) 
    && "Eindcijfer_VO" %in% df_top_vi$Variable[4:5]) {
  conclusion_text <- glue::glue(
    conclusion_text,
    "**Het model toont wel aan dat de herkomst van studenten sterker is gecorreleerd met",
    " {tolower(succes_model)} dan eerdere prestaties of vooropleiding.** ",
    "De _Variable Importance Factor_ (VI) laat namelijk zien dat de variabelen {top3_vi_list} ",
    "de 3 belangrijkste variabelen zijn voor het voorspellen van {tolower(sUitval_model)}, ",
    "gevolgd door {top45_vi_list}."
  )
} else if ("Eindcijfer_VO" %in% df_top_vi$Variable[1:3]) {
  conclusion_text <- glue::glue(
    conclusion_text,
    "**Het model toont aan dat eerdere prestaties van studenten sterker zijn gecorreleerd ",
    "met {tolower(succes_model)} dan herkomst.** ",
    "De _Variable Importance_ (VI) laat namelijk zien dat de variabelen {top3_vi_list} de ",
    "3 belangrijkste variabelen zijn voor het voorspellen van {tolower(succes_model)}, ",
    "gevolgd door {top45_vi_list}."
  )
} else {
  conclusion_text <- "**Vooralsnog geen nadere bijzonderheden.**"
}

```

### Het beste prognosemodel voor deze opleiding

**Het beste prognosemodel blijkt het `r best_model` model te zijn.**

-   Van de prognosemodellen die we hebben ontwikkeld om `r tolower(succes_model)` te voorspellen, had het `r best_model` model de hoogste AUC/ROC waarde (`r best_model_auc`).

### Mate van accuraatheid en lift

Een prognosemodel moet minimaal beter presteren dan een *advanced-report* om waarde op basis van accuraatheid toe te voegen. Het advanced-report neemt als basis de grootste klasse van de gemiddelde `r tolower(succes_model)` van de afgelopen jaren. Stel we zouden tegen alle studenten zeggen dat ze hun studie gaan halen, dan is de mate van accuratesse gelijk aan dit advanced-report. Dit advanced-report is dus altijd hoger dan de 50% lijn van de AUC/ROC curve, tenzij het advanced-report toevallig precies 50% is.


```{r}
#| label: fig-basemodel-lift
#| fig-cap: "Lift afhankelijk van advanced-report en accuraatheid"
#| out-width: 80%

knitr::include_graphics(here::here("R/images", "basemodel-lift.png"))

```

**De mate van accuraatheid van het prognosemodel is `r accuracy_level` (`r last_fit_accuracy`%).**

-   **Base model: `r base_model_accuracy`%** -- Voor deze opleiding berekenen we het base model als volgt. Van alle studenten studeerde `r avg_retention`% door; 100% - `r avg_retention`% = `r avg_non_retention`% studeerde *niet* door. De grootste klasse van deze twee, `r base_model_accuracy`%, is daarmee de accuratesse van het Base model.
-   **Accuratesse prognose: `r last_fit_accuracy`%** -- Het model voorspelt `r succes_model` met een accuratesse van `r last_fit_accuracy`%.
-   **Lift: `r accuracy_difference_percentage`** -- Het model scoort in de huidige opbouw met een verschil van `r accuracy_difference_percentage` (de *lift*) `r accuracy_difference` `r accuracy_comparison` dan de accuraatheid van het advanced-report.

### Confusion Matrix

```{r}
#| label: confusion-matrix-calculation

# Determine the confusion matrix
confusion_matrix <- last_fit |>
  collect_predictions() |>
  conf_mat(truth = Retentie, estimate = .pred_class) 

df_confusion_matrix <- as_tibble(confusion_matrix$table) |>
  rename(Werkelijkheid = Truth) |>
  mutate(Werkelijkheid = ifelse(Werkelijkheid == "TRUE", "Retentie", "Geen retentie"),
         Prediction    = ifelse(Prediction == "TRUE", "Retentie", "Geen retentie"))

tp_percentage  <- change_number_marks((df_confusion_matrix$n[4] / 
                                         sum(df_confusion_matrix$n) * 100), 1)
fp_percentage  <- change_number_marks((df_confusion_matrix$n[2] / 
                                         sum(df_confusion_matrix$n) * 100), 1)
tn_percentage  <- change_number_marks((df_confusion_matrix$n[1] / 
                                         sum(df_confusion_matrix$n) * 100), 1)
fn_percentage  <- change_number_marks((df_confusion_matrix$n[3] / 
                                         sum(df_confusion_matrix$n) * 100), 1)
acc_percentage <- change_number_marks(last_fit_accuracy, 1)

```

De prestaties van het model kunnen we verder uitdrukken in een *confusion matrix*. Hierin zien we de voorspellingen van het model en de werkelijke uitkomsten. De matrix geeft inzicht in de mate van correcte en incorrecte voorspellingen. Ter illustratie werken we de matrix uit voor een voorspelling waarop een bindend studieadvies (BSA) gebaseerd zou kunnen zijn.

```{r}
#| label: fig-confusion-matrix-explanation
#| fig-cap: "Confusion matrix in relatie tot BSA"

knitr::include_graphics(here::here("R/images", "confusion-matrix-retention-lta-hhs.png"))

```

We passen de confusion matrix nu toe op het model dat als beste naar voren kwam. De **accuraatheid** van dit model is **`r acc_percentage`%**. De accuraatheid van het model berekenen we door de som van de diagonaal te berekenen: het aandeel goed voorspelde uitkomsten, Retentie = Retentie (*True Positive*) en Geen retentie = Geen retentie (*True Negative*), af te zetten tegen het totaal aantal voorspellingen: `r tp_percentage`% + `r tn_percentage`% = `r acc_percentage`%. (NB. De weergave in deze confusion matrix is diagonaal gespiegeld vergeleken met het voorbeeld.)

```{r}
#| label: fig-confusion-matrix-actual
#| fig-cap: !expr 'paste("Confusion matrix ten opzichte van", params$succes)'
 
confusion_plot <- cvms::plot_confusion_matrix(df_confusion_matrix,
                                        target_col = "Werkelijkheid",
                                        prediction_col = "Prediction",
                                        counts_col = "n",
                                        palette = "Blues",
                                        add_sums = TRUE,
                                        theme_fn = ggplot2::theme_light,
                                        sums_settings = cvms::sum_tile_settings(
                                          palette = "Greens",
                                          label = "Totaal",
                                          tc_tile_border_color = "black"
                                        )) +
    
  # Customize the labels
  labs(
    x = "Werkelijke uitkost",
    y = "Voorspelde uitkomst",
    caption = caption
  ) +
  
  set_theme()
  
# Add theme elements
confusion_plot <- add_theme_elements(confusion_plot, title_subtitle = TRUE)

print(confusion_plot)
  
```

### Uitleggen of verklaren?

Naast de accuraatheid van het model is het ook belangrijk om te weten welke factoren het meest bijdragen aan de voorspelling van `r tolower(succes_model)`. Daarin gaat de vergelijking met de prestaties van het basemodel mank. Dat model geeft op geen enkele manier aan waarom een student een kans op succes heeft, anders dan - 'dit is gebruikelijk in deze opleiding'.

Ongeacht de mate van accuraatheid, is het voor onderzoek naar kansengelijkheid essentieel om te weten welke factoren het meest bijdragen aan de voorspelling van `r tolower(succes_model)`. Het gaat erom dat we het belang van de factoren in de voorspellingen kunnen begrijpen en duiden. Machine Learning is hiervoor uitstekend geschikt, omdat het de mogelijkheid biedt om de belangrijkste factoren en hun invloed te leren kennen [@Shmueli.2010; @Shmueli.2011].

<!-- -   `r conclusion_text` -->

<!-- FOOTER -->

<!-- Justification -->

::: {.content-hidden unless-meta="includes.justification"}
{{< include R/qmd/footer-justification.qmd >}}
:::

<!-- Copyright -->

::: {.content-hidden unless-meta="includes.copyright"}
{{< include R/qmd/footer-copyright.qmd >}}
:::

<!-- Cleaning up -->

```{r}
#| label: cleanup
#| echo: false

# Datasets
rm(
  df_sp_enrollments,
  df_summary,
  df_retention_test,
  df_retention_train,
  df_retention_validation,
  splits
)

# Logistic regression
if (include_model_lr) {
  rm(
    lr_auc,
    lr_auc_highest,
    lr_best,
    lr_mod,
    lr_plot,
    lr_recipe,
    lr_res,
    lr_workflow
  )
  # if (best_model == "lr") {
  #   rm(last_lr_mod,
  #      last_lr_workflow)
  # }
}

# Random Forest
if (include_model_rf) {
  rm(
    rf_auc,
    rf_auc_highest,
    rf_best,
    rf_mod,
    rf_recipe,
    rf_res,
    rf_workflow,
    last_rf_mod,
    last_rf_workflow
  )
  # if (best_model == "rf") {
  #   rm(last_rf_mod,
  #      last_rf_workflow)
  # }
}

# Final Fit
if (include_final_fit) {
  rm(
    last_fit
  )
}

# Conclusions
if (include_conclusions) {
  rm(
    top_models,
    df_top_vi,
    top3_vi_list,
    top45_vi_list
  )
}

# Collect garbage
invisible(gc())
```
